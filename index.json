[{"authors":["admin"],"categories":null,"content":"I am currently working on developing foundational models for Amazon Advertising. Previously, I focused on building multilingual models to moderate ads within Amazon Sponsored Products. My experience spans a wide range of Reinforcement Learning (RL) applications, including optimizing supply chains, cost-to-serve models, and dialogue generation. I have also contributed to improving the robustness and sample efficiency of RL algorithms. Beyond RL, I have worked at the intersection of Natural Language Processing (NLP) and Deep Learning, with a particular focus on sentiment analysis.\nI am also interested in collaborations in the mentioned areas below. If you have an interesting project, and would like to collaborate or talk, or just want to talk about research in general, please reach out to me.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently working on developing foundational models for Amazon Advertising. Previously, I focused on building multilingual models to moderate ads within Amazon Sponsored Products. My experience spans a wide range of Reinforcement Learning (RL) applications, including optimizing supply chains, cost-to-serve models, and dialogue generation.","tags":null,"title":"Hardik Meisheri","type":"authors"},{"authors":null,"categories":null,"content":"Building Full-Fledged Demos with LLMs: Lessons from the Trenches When you\u0026rsquo;re working with large language models (LLMs) to bring an idea to life, it\u0026rsquo;s easy to be impressed by how quickly they can write code, generate explanations, or stitch together components. But once you try to build something that actually works end-to-end like a real demo with frontend, backend, integrations, and maybe even some bells and whistles the experience shifts dramatically.\nRecently, I decided to push a fun idea all the way to a functioning demo using LLMs as my main assistants. I relied on them to suggest code, fix bugs, structure the project, and more. Along the way, I discovered what works, what really does not, and a few key practices that made the whole process smoother.\nHere are some lessons I learned the hard way, so you hopefully do not have to.\n1. Always Ask for a Directory Structure Right at the Start Before you write a single line of code, ask the LLM to give you a full directory structure. This might seem like an obvious step, but it sets the stage for everything that follows. Without it, your project will likely end up feeling like a messy desk drawer where nothing is in the right place.\nMake the model walk through what folders are needed, what files should go in each folder, and even ask it to include placeholder comments inside the files. This helps you and the model stay aligned on the architecture and responsibilities of each part of the system.\nYou are not just writing code. You are designing a system. Get that system layout early and refer back to it often.\n2. Regenerate Full Files Every Few Steps to Avoid Drift LLMs are clever, but they can lose track of the big picture pretty quickly. As conversations go on, they might forget earlier details or start to contradict themselves without realizing it.\nOne of the best things you can do is to ask the model to regenerate the entire file after every three or four rounds of updates or modifications. It may feel redundant, but this helps keep everything internally consistent.\nWithout this, you will find yourself fixing logic errors that crept in because a function name changed slightly, or an assumption made earlier was silently dropped.\nTreat this like refreshing the page. It helps you avoid stale or corrupted states.\n3. When the LLM Gets Stuck, Step In and Debug Manually There will be moments when the LLM just cannot seem to figure out what is wrong. It will keep suggesting similar fixes, none of which solve the problem. This is your cue to stop relying on the model and start debugging like a human.\nHere’s what worked for me:\n I made a list of all the potential failure modes, even the unlikely ones. I asked the LLM to walk through each one step by step, instead of guessing. I clearly told the model where I had already looked, so it did not waste time going in circles.  In these situations, your job is to guide the model with precision. If you do not, it will happily spin its wheels forever.\nThink of yourself as the engineer, and the LLM as your very eager intern who needs clear direction.\n4. Respect the Context Limit and Plan Accordingly LLMs operate within a limited context window, which is essentially the amount of information they can keep track of at once. This limit is somewhere around fifty thousand tokens, depending on the model. Once you cross this threshold, things start to break. The model forgets earlier parts of the conversation, or injects bugs that seem to come out of nowhere.\nYou need to plan your workflow with this in mind.\nBreak down your project into self-contained milestones that can each be completed within a single session or context. When you hit the limit, start a fresh chat. Summarize what you have done so far, maybe even paste some key files or notes, and continue from there.\nThis avoids the haunted project problem where bugs appear for no reason because the model lost track of something important two hundred messages ago.\n5. For Authentication and Security, Trust External Resources One major weak spot for LLMs is anything related to authentication, security, or secure integration. They will give you code, yes, but it is often outdated, insecure, or just plain wrong.\nI learned to ask the LLM for a rough sketch of what I needed like the general flow of an auth system or how tokens should be passed but I never trusted it with the implementation.\nInstead, I looked up official docs, used battle-tested services like Firebase or Auth0, and followed community best practices.\nSecurity is one area where human oversight is absolutely essential.\nTheory Backs It Up: Why LLMs Behave Like This As it turns out, some of the odd behaviors I encountered while working with LLMs are not just quirks or artifacts of incomplete training—they are actually rooted in the underlying mechanics of how these models process sequences.\nI recently came across a fascinating paper by Barbero et al. (2025) titled Why do LLMs attend to the first token?, which explores a phenomenon known as attention sinks. In essence, many transformer-based language models exhibit a behavior where a disproportionately high amount of attention is directed toward the very first token in the sequence. This is often the special \u0026lt;bos\u0026gt; or beginning-of-sequence token.\nAt first glance, this may seem wasteful or like a design flaw. However, the authors present a compelling argument that this behavior is not only intentional but also serves a critical role in how LLMs manage information across long contexts.\nTheir main idea is that LLMs are prone to what is known as over-mixing or representational collapse. When a model processes a long sequence or operates at great depth, it tends to homogenize the representations of tokens. In simpler terms, the distinctiveness of individual tokens starts to get lost. This leads to a phenomenon where all token embeddings start to look the same, which makes the model forgetful or erratic.\nTo counteract this, the model learns to create an anchor by heavily attending to the first token. This acts like a stabilizer that slows down how quickly information gets mixed and propagated through the network. It is a self-learned mechanism to retain structure and clarity in representations, especially in very long sequences.\nNow, here is the fun part. In my experience, I noticed that regenerating entire files periodically, restarting sessions after long stretches of interaction, and manually identifying the source of bugs were all very effective strategies. At the time, these felt like pragmatic hacks. But when viewed through the lens of this paper, they look a lot more like intuitive workarounds for problems that the model itself is also trying to solve.\nFor example:\n My habit of regenerating full files every few turns aligns with the idea of resisting representational collapse. By starting fresh, I was forcing the model to re-encode the correct structure and logic, preventing subtle drift. Restarting the session after approximately fifty thousand tokens of interaction helped avoid vestigial context errors, which the paper describes as a symptom of over-squashing and token signal decay. When the model got stuck suggesting the same broken fix repeatedly, it was often a result of it being too anchored to earlier tokens or unable to distinguish between closely mixed representations. Taking over the debugging process and directing its focus elsewhere gave it the nudge it needed to break free from that loop.  In hindsight, I was not just patching things up. I was, unknowingly, applying a form of interpretability-aware engineering. The theoretical research just validated it.\nThis paper provided a comforting sense of clarity. It explained why the LLM was doing what it was doing and reassured me that the workarounds I used were not just stopgaps—they were intelligent responses to real, measurable behaviors in large language models.\nAs it turns out, some of the odd behaviors I encountered while working with LLMs are not just quirks or artifacts of incomplete training—they are actually rooted in the underlying mechanics of how these models process sequences.\nI recently came across a fascinating paper titled \u0026ldquo;Why do LLMs attend to the first token?\u0026quot; which explores a phenomenon called attention sinks. In essence, many transformer-based language models exhibit a behavior where a disproportionately high amount of attention is directed toward the very first token in the sequence. This is often the special \u0026lt;bos\u0026gt; or beginning-of-sequence token.\nAt first glance, this may seem wasteful or like a design flaw. However, the authors present a compelling argument that this behavior is not only intentional but also serves a critical role in how LLMs manage information across long contexts.\nTheir main idea is that LLMs are prone to what is known as over-mixing or representational collapse. When a model processes a long sequence or operates at great depth, it tends to homogenize the representations of tokens. In simpler terms, the distinctiveness of individual tokens starts to get lost. This leads to a phenomenon where all token embeddings start to look the same, which makes the model forgetful or erratic.\nTo counteract this, the model learns to create an anchor by heavily attending to the first token. This acts like a stabilizer that slows down how quickly information gets mixed and propagated through the network. It is a self-learned mechanism to retain structure and clarity in representations, especially in very long sequences.\nNow, here is the fun part. In my experience, I noticed that regenerating entire files periodically, restarting sessions after long stretches of interaction, and manually identifying the source of bugs were all very effective strategies. At the time, these felt like pragmatic hacks. But when viewed through the lens of this paper, they look a lot more like intuitive workarounds for problems that the model itself is also trying to solve.\nFor example:\n My habit of regenerating full files every few turns aligns with the idea of resisting representational collapse. By starting fresh, I was forcing the model to re-encode the correct structure and logic, preventing subtle drift. Restarting the session after approximately fifty thousand tokens of interaction helped avoid vestigial context errors, which the paper describes as a symptom of over-squashing and token signal decay. When the model got stuck suggesting the same broken fix repeatedly, it was often a result of it being too anchored to earlier tokens or unable to distinguish between closely mixed representations. Taking over the debugging process and directing its focus elsewhere gave it the nudge it needed to break free from that loop.  In hindsight, I was not just patching things up. I was, unknowingly, applying a form of interpretability-aware engineering. The theoretical research just validated it.\nThis paper provided a comforting sense of clarity. It explained why the LLM was doing what it was doing and reassured me that the workarounds I used were not just stopgaps—they were intelligent responses to real, measurable behaviors in large language models.\nWrapping Up If you are trying to go from idea to working demo using LLMs, you are going to hit roadblocks. But with the right strategies, you can avoid many of the common pitfalls and move much faster.\nUse the LLM as a powerful assistant, not as an all-knowing oracle. Guide it, correct it, and do not be afraid to take the wheel when needed.\nLet the models do the heavy lifting, but remember. You are still the one steering the ship.\nReferences Barbero, F., Arroyo, Á., Gu, X., Perivolaropoulos, C., Bronstein, M., Velicković, P., \u0026amp; Pascanu, R. (2025). Why do LLMs attend to the first token? arXiv preprint arXiv:2504.02732.\n","date":1743724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743724800,"objectID":"e68aaf353daababd812d1f9976add303","permalink":"/post/2025-04-04-lessons_llm/","publishdate":"2025-04-04T00:00:00Z","relpermalink":"/post/2025-04-04-lessons_llm/","section":"post","summary":"Building Full-Fledged Demos with LLMs: Lessons from the Trenches When you\u0026rsquo;re working with large language models (LLMs) to bring an idea to life, it\u0026rsquo;s easy to be impressed by how quickly they can write code, generate explanations, or stitch together components.","tags":["Blogs","ML"],"title":"Musings While Building with LLMs","type":"post"},{"authors":["Hardik Meisheri","Somjit Nath","Mayank Baranwal","Harshad Khadilkar"],"categories":null,"content":"","date":1646179200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646179200,"objectID":"69bd6967e1f1451122965a684e326fbb","permalink":"/publication/a_learning_based_framework_for_handling_uncertain_lead_times_in_multi-product_inventory_management/","publishdate":"2022-03-02T00:00:00Z","relpermalink":"/publication/a_learning_based_framework_for_handling_uncertain_lead_times_in_multi-product_inventory_management/","section":"publication","summary":"Most existing literature on supply chain and inventory management consider stochastic demand processes with zero or constant lead times. While it is true that in certain niche scenarios, uncertainty in lead times can be ignored, most real-world scenarios exhibit stochasticity in lead times. These random fluctuations can be caused due to uncertainty in arrival of raw materials at the manufacturer's end, delay in transportation, an unforeseen surge in demands, and switching to a different vendor, to name a few. Stochasticity in lead times is known to severely degrade the performance in an inventory management system, and it is only fair to abridge this gap in supply chain system through a principled approach. Motivated by the recently introduced delay-resolved deep Q-learning (DRDQN) algorithm, this paper develops a reinforcement learning based paradigm for handling uncertainty in lead times action delay. Through empirical evaluations, it is further shown that the inventory management with uncertain lead times is not only equivalent to that of delay in information sharing across multiple echelons observation delay, a model trained to handle one kind of delay is capable to handle delays of another kind without requiring to be retrained. Finally, we apply the delay-resolved framework to scenarios comprising of multiple products subjected to stochasticity in lead times, and elucidate how the delay-resolved framework negates the effect of any delay to achieve near-optimal performance.","tags":["Reinforcement Learning","Supply Chain","Deep Learning"],"title":"A Learning Based Framework for Handling Uncertain Lead Times in Multi-Product Inventory Management","type":"publication"},{"authors":["Somjit Nath","Omkar Shelke","Durgesh Kalwar","Hardik Meisheri","Harshad Khadilkar"],"categories":null,"content":"","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"813dfbebe775558df888a6d05e9c4c9d","permalink":"/publication/follow_your_nose_using_general_value_functions_for_directed_exploration_in_reinforcement_learning/","publishdate":"2022-02-28T00:00:00Z","relpermalink":"/publication/follow_your_nose_using_general_value_functions_for_directed_exploration_in_reinforcement_learning/","section":"publication","summary":"Exploration versus exploitation dilemma is a significant problem in reinforcement learning (RL), particularly in complex environments with large state space and sparse rewards. When optimizing for a particular goal, running simple smaller tasks can often be a good way to learn additional information about the environment. Exploration methods have been used to sample better trajectories from the environment for improved performance while auxiliary tasks have been incorporated generally where the reward is sparse. If there is little reward signal available, the agent requires clever exploration strategies to reach parts of the state space that contain relevant sub-goals. However, that exploration needs to be balanced with the need for exploiting the learned policy. This paper explores the idea of combining exploration with auxiliary task learning using General Value Functions (GVFs) and a directed exploration strategy. We provide a simple way to learn options (sequences of actions) instead of having to handcraft them, and demonstrate the performance advantage in three navigation tasks.","tags":["Reinforcement Learning","Deep Learning"],"title":"Follow your Nose: Using General Value Functions for Directed Exploration in Reinforcement Learning","type":"publication"},{"authors":["Hardik Meisheri","Harshad Khadilkar"],"categories":null,"content":"","date":1632268800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632268800,"objectID":"5aa8f508987d20e54f01fa2c942a5828","permalink":"/publication/folar/","publishdate":"2021-09-22T00:00:00Z","relpermalink":"/publication/folar/","section":"publication","summary":"We propose a novel methodology for improving the rate and consistency of reinforcement learning in partially observable (foggy) environments, under the broader umbrella of robust latent representations. The present work addresses partially observable environments, which violate the canonical Markov assumptions. We propose adaptations for any on-policy model-free deep reinforcement learning algorithm, in order to improve training in partially observable situations (i) recurrent layers for including information from previous observations, (ii) predicting the step reward and the next latent representation as auxiliary outputs from the same latent space as used for inferring the action, and (iii) modification of the loss function to penalise errors in the two auxiliary outputs, in addition to the reward-based gradients used for policy training. We show that the proposed changes substantially improve learning in several environments over vanilla Proximal Policy Optimisation (PPO) and other baselines in literature, especially in known challenging environments with hard exploration.","tags":["Reinforcement Learning","Deep Learning"],"title":"FoLaR: Foggy Latent Representations for Reinforcement Learning with Partial Observability","type":"publication"},{"authors":["Hardik Meisheri","Nazneen N Sultana","Mayank Baranwal","Vinita Baniwal","Somjit Nath","Satyam Verma","Balaraman Ranvindran","Harshad Khadilkar"],"categories":null,"content":"","date":1622160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622160000,"objectID":"68fc15b9e1fcda90e31e9ea74427fd34","permalink":"/publication/scalable_multi-product_inventory_control_with_lead_time_constraints_using_reinforcement_learning/","publishdate":"2021-05-28T00:00:00Z","relpermalink":"/publication/scalable_multi-product_inventory_control_with_lead_time_constraints_using_reinforcement_learning/","section":"publication","summary":"Determining optimum inventory replenishment decisions is critical for retail businesses with uncertain demand. The problem becomes particularly challenging when multiple products with different lead times and cross-product constraints are considered. This paper addresses the aforementioned challenges in multi-product, multi-period inventory management using deep reinforcement learning (deep RL). The proposed approach improves upon existing methods for inventory control on three fronts (i) concurrent inventory management of a large number (hundreds) of products under realistic constraints, (ii) minimal retraining requirements on the RL agent under system changes through the definition of an individual product meta-model, (iii) efficient handling of multi-period constraints that stem from different lead times of different products. We approach the inventory problem as a special class of dynamical system control, and explain why the generic problem cannot be satisfactorily solved using classical optimisation techniques. Subsequently, we formulate the problem in a general framework that can be used for parallelised decision-making using off-the-shelf RL algorithms. We also benchmark the formulation against the theoretical optimum achieved by linear programming under the assumptions that the demands are deterministic and known apriori. Experiments on scales between 100 and 220 products show that the proposed RL-based approaches perform better than the baseline heuristics, and quite close to the theoretical optimum. Furthermore, they are also able to transfer learning without retraining to inventory control problems involving different number of products.","tags":["Reinforcement Learning","Supply Chain","Deep Learning"],"title":"Scalable multi-product inventory control with lead time constraints using reinforcement learning","type":"publication"},{"authors":["Hardik Meisheri","Harshad Khadilkar"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"bfd07850b44dde24cb052eea4c25893e","permalink":"/publication/sample_efficient_training_in_multi-agent_adversarial_games_with_limited_teammate_communication/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/sample_efficient_training_in_multi-agent_adversarial_games_with_limited_teammate_communication/","section":"publication","summary":"We describe our solution approach for Pommerman TeamRadio, a competition environment associated with NeurIPS 2019. The defining feature of our algorithm is achieving sample efficiency within a restrictive computational budget while beating the previous years learning agents. The proposed algorithm (i) uses imitation learning to seed the policy,(ii) explicitly defines the communication protocol between the two teammates,(iii) shapes the reward to provide a richer feedback signal to each agent during training and (iv) uses masking for catastrophic bad actions. We describe extensive tests against baselines, including those from the 2019 competition leaderboard, and also a specific investigation of the learned policy and the effect of each modification on performance. We show that the proposed approach is able to achieve competitive performance within half a million games of training, significantly faster than other studies in the literature.","tags":["Reinforcement Learning","Games","Pommerman","Deep Learning"],"title":"Sample Efficient Training in Multi-Agent Adversarial Games with Limited Teammate Communication","type":"publication"},{"authors":null,"categories":null,"content":"List of blogs are informative for RL/ML\n  Machine Learning Blog at CMU  Berkeley Artificial Intelligence Research Blog  Standford AI Blog  Lilian Weng blog on RL  Deep Mind Blog  Lectures/courses for Machine Learning\n  Create machine learning models - Microsoft  Stanford CS229: Machine Learning - Andrew Ng  Linear Regression and Gradient Descent Logistic Regression Naive Bayes SVMs Kernels Decision Trees Introduction to Neural Networks Debugging ML Models    Machine Learning Crash Course - Google  Introduction to Machine Learning for Coders - Jeremy Howard  Foundations of Machine Learning - Bloomberg ML EDU  Tabular Data - Machine Learning University  Stat 451: Intro to Machine Learning (Fall 2020) - Sebastain Raschka  Deep Mind x UCL, Reinforcement Learning  NYU Deep Learning SP21  Neural Nets: rotation and squashing Latent Variable Energy Based Models Unsupervised Learning Generative Adversarial Networks Autoencoders    ","date":1600646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600646400,"objectID":"18fa5204aeaa0f71f3e322f6df5c7d92","permalink":"/post/2020-09-21-interesting_blogs/","publishdate":"2020-09-21T00:00:00Z","relpermalink":"/post/2020-09-21-interesting_blogs/","section":"post","summary":"List of blogs are informative for RL/ML\n  Machine Learning Blog at CMU  Berkeley Artificial Intelligence Research Blog  Standford AI Blog  Lilian Weng blog on RL  Deep Mind Blog  Lectures/courses for Machine Learning","tags":["Blogs","ML"],"title":"Useful courses/blogs related to Machine Learning","type":"post"},{"authors":["Hardik Meisheri","Vinita Baniwal","Nazneen N Sultana","Harshad Khadilkar","Balaraman Ranvindran"],"categories":null,"content":"","date":1588982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588982400,"objectID":"2325b88e96af5e51bf222b8b2b2e6069","permalink":"/publication/using_reinforcement_learning_for_a_large_variable-dimensional_inventory_management_problem/","publishdate":"2020-05-09T00:00:00Z","relpermalink":"/publication/using_reinforcement_learning_for_a_large_variable-dimensional_inventory_management_problem/","section":"publication","summary":"This paper evaluates the applicability of reinforcement learning (RL) to multi-product inventory management in supply chains. The novelty of this problem with respect to supply chain literature is (i) we consider concurrent inventory management of a large number (hundreds) of products under realistic constraints such as shared capacity, and (ii) the number of products (size of the problem) can change frequently, implying that the RL agent needs to work in this regime without retraining. We approach the problem as a special class of dynamical system control, and explain why the generic problem cannot be satisfactorily solved using classical optimisation techniques. Subsequently, we formulate the problem in a reinforcement learning framework that can be used for parallelised decision-making, and use the advantage actor critic (A2C) and deep Q-network (DQN) algorithms with quantised action spaces to solve the problem. Experiments on scales between 100 and 220 products show that these approaches perform better than other baseline algorithms. They are also able to transfer learning without retraining, when the number of products change.","tags":["Reinforcement Learning","Supply Chain","Deep Learning"],"title":"Using Reinforcement Learning for a Large Variable-Dimensional Inventory Management Problem","type":"publication"},{"authors":null,"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"382c7273589324024e73e1c9d2a63f8e","permalink":"/project/sample-efficient-rl/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/project/sample-efficient-rl/","section":"project","summary":"Making Model-free RL sample efficient","tags":["Reinforcement Learning","Deep Learning"],"title":"Sample Efficient RL","type":"project"},{"authors":["Sachin Thukral","Arnab Chatterjee","Hardik Meisheri","Tushar Kataria","Aman Agarwal","Ishan Verma","Lipika Dey"],"categories":null,"content":"","date":1577491200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577491200,"objectID":"eeaaf2a41584b8b712c531091a122801","permalink":"/publication/characterizing_behavioral_trends_in_a_community_driven_discussion_platform/","publishdate":"2019-12-28T00:00:00Z","relpermalink":"/publication/characterizing_behavioral_trends_in_a_community_driven_discussion_platform/","section":"publication","summary":"This article presents a systematic analysis of the patterns of behavior of individuals as well as groups observed in community-driven platforms for discussion like Reddit, where users usually exchange information and viewpoints on their topics of interest. We perform a statistical analysis of the behavior of posts and model the users’ interactions around them. A platform like Reddit which has grown exponentially, starting from a very small community to one of the largest social networks, with its large user base and popularity harboring a variety of behavior of users in terms of their activity. Our work provides interesting insights about a huge number of inactive posts which fail to attract attention despite their authors exhibiting Cyborg-like behavior to attract attention. We also observe short-lived yet extremely active posts emulate a phenomenon like Mayfly Buzz. A method is presented, to study the activity around posts which are highly active, to determine the presence of Limelight hogging activity. We also present a systematic analysis to study the presence of controversies in posts. We analyzed data from two periods of 1-year duration but separated by few years in time, to understand how social media has evolved through the years.","tags":["Behavioral Analysis","Reddit"],"title":"Characterizing Behavioral Trends in a Community Driven Discussion Platform","type":"publication"},{"authors":null,"categories":null,"content":"I presented my work on Pommerman at Deep Reinforcement Learning Workshop in NeurIPS this year, titled \u0026ldquo;Accelerating training in Pommerman with Imitation and Reinforcement Learning\u0026rdquo;, coauthors are Omkar Shelke, Richa Verma and Harshad Khadilkar. The selection of talks and posters to be visited was done primarily for Reinforcement Learning and to some extent NLP. Due to high rush during the poster session (one has to wait in queue for 30 minutes to enter poster session) and 4 parallel tracks during presentation, I have most probably seen 20-25% of the conference session. The main highlight for me was the socials which were introduced this year by NeurIPS for having informal discussions with prominent people in the field. I attended RL social and had a detailed discussion with Richard Sutton, David Silver, Martha White, Michael Littman etc, over topics ranging from causality in RL to going away with the MDP settings in the RL. All in all, it was an enlightening experience to have a deep philosophical discussion, especially with Sutton. The common notion that resonated with all of them was to have a big picture in mind, before delving down to a very specific problem. If building an AGI is the ultimate goal, try to place your work in that context and work from there.\nIn RL, people have been looking at sample efficient RL, batch RL, meta-learning and towards ablation studies of existing algorithms. It would not be correct to gauge the overall direction of community as I did not attend all of the talks, going through the papers and posters I did find that lot of work has been towards having much more deeper understanding towards the theory of deep learning. Talks and most of the workshops were recorded and recordings are available at https://slideslive.com/neurips/ . This report is short summary of some of the talks/tutorials I attended, and a list of things that I found interesting during the conference.\n Tutorials Imitation learning and its application to Natural Language Generation   Given by: Kyunghyun Cho, Hal Daume III \nThe tutorial focuses on using imitation learning and reinforcement learning in NMT (Neural Machine Translation), dialogues and story generation. The main challenge in the using Beam search for searching is lack of diversity and in the literature mostly it is tackled by adding noise. Reinforcement learning with its stochastic policies can have a great improvement here, especially when we want to have more natural dialogues, stories which are not similar yet have a structure of plots and make sense etc. The story examples given by the authors were really impressive.\nEfficient Processing of Deep Neural Network: from Algorithm to Hardware  Given by: Vivienne Sze \nThe tutorial provides insights into designing efficient hardware for DL. They focus on different constraints such as speed, latency, energy consumption and cost. All of these constraints require a careful trade-off between each other. For example as shown in the figure below, where we can increase the speed of convolution at the expense of data duplication. They discuss architecture such as CPU and GPU to task-specific architectures of FPGA. Most of the presentation was focused on optimizing the hardware for Image processing and vision task.\nReinforcement Learning: Past, Present and Future Perspectives Given by: Katja Hofmann This was a very elaborate session of RL, from basic MDP formulation to Multi-agent RL with a different perspective in generalization and evaluating policies. Minecraft case study was very interesting for long term reward and exploration.\nInvited Talks/Keynotes Celeste Kidd: How to know  Questions: \n How do we know, what we know How do two people living in the same world come to believe very different things Why do people believe somethings which aren\u0026rsquo;t true Age of information and also misinformation How do we form Belief Vast amount of knowledge present in the world, an individual learner has to pick and choose what to learn and how to learn, how does one decide that  We need to answer all these questions because we will need to build much more accurate models of the world and have this knowledge embedded in the AI agents. Her work is around mostly inferencing on what babies infer in the real world, what do they look, where to they focus, what kind of prediction they make. Similar work was discussed by Josh Tannenbaum in his last year\u0026rsquo;s ICML talk\n Five Key things \n Humans continuously form beliefs (It\u0026rsquo;s not a one-step process) \u0026ndash;\u0026gt; think of it as a probabilistic expectation. (Humans tend to lose attention if things are too predictable or too surprising, picking a book which one knows (redundancy) is boring to read whereas picking a book written in the language which you don\u0026rsquo;t even understand (absolute new information) is also not fun to read) Certainty diminishes interest, an agent should be smart enough to not learn and waste resources on which it is certain about, humans do that. However, we are often wrong about things which we are certain about. We have a very poor model of our own uncertainty. Certainty is driven by feedback. When feedback is not available our feedback might be way off. Less feedback may encourage overconfidence. Small confirmatory feedback is enough to solidify some concepts and lead to wrong assumptions about concepts. Given the diversity in the concepts for the same object, are people aware of the weird behaviour about the notion of the concept that they have. Humans form beliefs quickly. The algorithms pushing content online have profound impacts on what we believe.  She talked about the work toxicity for women and how small instances of false accusation has created a fear among men. She emphasized when women complains and talks about some bad things that she has gone through, chances are she as went through much more than that. Standing Ovation by the crowd.\nTest of time, Lin Xiao: Dual Averaging Method for Regularized Stochastic Learning and Online Optimization Two main ideas which were dominant/promising direction in 2009, Stochastic Gradient Descent and Online Convex Optimization. Other important works during early 2000\u0026rsquo;s compressed sensing/sparse optimization, interior point methods, proximal gradient methods. Paper explores combining SGD with sparse optimization.\nYoshua Bengio: From System 1 Deep Learning to System 2 Deep Learning  Motivation \nQuestions: is it enough to have more data, compute, large models? How far are we from Human-level AI. We need to think more tangential to the DL paradigm. Inspiration: “Thinking Fast and Slow” by Daniel Kahneman. System-1,2 is also same as describe in podcast of Sean Carroll System 1 \u0026ndash;\u0026gt; Intuitive, fast unconscious non-linguistic, and habitual decision making/inference. (DL is good at this) System 2 \u0026ndash;\u0026gt; Slow, logical, sequential, conscious, linguistic, algorithmic,decision making. (DL is not equipped to deal with this)\n Things missing in DL \n Out of distribution generalization and transfer High level cognition: need to move from system 1 to system 2 \u0026ndash;\u0026gt; High level semantic representation, causality Machines that understand the world by building world models, Knowledge seeking mechanism   Talk \n ML Goal: Handle Changes in Distribution System 2 Basics: Attention and Consciousness Consciousness Prior: Sparse Factor Graph Theoretical framework: meta-learning, localized change hypothesis causal discovery Compositional DL architectures  Michael Littman on Assessing the Robustness of Deep RL Algorithms Motivation for this talk was to understand, why any decisions are taken by a policy in certain scenarios (DQN for atari as example), build a Saliency map by masking of portions of state space. Conclusion being, DQN does not learn the statespace as we see it, although it has strategies to win the game it does not understand the game per say. Evaluation metrics for generalization Value Estimation Error, total accumulated reward.\nOral/spotlight Presentation Towards Explaining the regularization effect of initial large learning rate in training neural networks Authors explain the effect of learning rate on patterns that are learnt. They prove that having a lower learning rate learns very myopic structure, where as larger learning rate learns the overall/macro structure. They prove that over a CIFAR dataset, where they super-impose a patch of image over the original image in the dataset. The Image from the dataset can be considered to hard-to-generalize but easy-to-fit sample, whereas small patch can be considered as easy-to-generalize but hard-to-fit sample.\nSample Complexity of Deep Neural Networks via Lipschitz Augmentation Objective is to prove upper bounds on generalization error and improve with regularization. The norm of weights is not sufficient for getting upper bounds, instead make it is a function of data.\nOutstanding New Directions Paper: Uniform Convergence may be unable to explain generalization in deep learning Objective of this work was to understand why does over-parameterized networks generalize well. The generalization error or the bound does not just depend on the parameters but also on the training set size. Uniform convergence does not explain the generalization.\nOn Exact Computation with an Infinitely Wide Neural Net Work was based on relationship between the width of the Neural network and neural tangent kernel. On the lines of understanding larger width networks can achieve zero prediction errors. NTK performance is comparable to CNNs.\nGeneralization Bounds of SGD for Wide and Deep NNs Key ideas: Deep ReLu nets are almost linear in parameters in a small neighborhood around initialization, Loss is Lipschitz continuous and almost convex\nCausal Confusion in Imitation Learning Imitation learning: powerful mechanism for learning complex behaviors. Behavioral cloning is imitation learning with supervised settings. But the imitation policy fails the moment it encounters state which it has not seen during behavior cloning, and there is catastrophic degradation in the policies (Distributional shift in the data). Does more information mean better performance \u0026ndash;\u0026gt; Not necessarily\nBehavior cloning learns to brake when the brake light is on!!!! Just having the information about the road is more generalized. Giving more information leads to confusion. Ways to tackle is predict actions conditioned over causes and Targeted intervention by expert queries.\nImitation Learning from Observations by Minimizing Inverse Dynamics Disagreement This work was on imitation learning when you need to learn the expert policy with just state/observations. They argue that the difference between the imitation learning and learning with just state spaces is proportional in the disagreement of actions in the actions and expert (GAIL).\nLearning to Control Self-Assembling Morphologies: A study of Generalization via Modularity Authors propose to have lego kind of modules with each module taking inputs+message and generating outputs + message. This settings can lead to much more robust policies in co-operative environment. Co-Evolution of control and morphology. Policies are shared along the modules and hence they learn much more robust policies. Link to the video https://youtu.be/cg-RdkPtRiQ, last part of the video where, it accomplishes the task even with losing some modules is spooky. A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning Objective: Find policies that generalize to new agent-task pairs not seeing during training. Compute score on previous pairs, using structured inference, use this to infer new policy on new pairs. Add a new constraint to the optimization to make it a linear program. The constraint encourages agents to cooperate. Further extend the LP to a quadratic program.\nGuided Meta-Policy Search RL is very sample inefficient. Meta-Learning is one possible solution to this problem by making efficent use of previous training tasks. Most meta-RL methods also requires huge amounts of data for the training phase, even if the adaptation is efficient. Also shortcomings in exploration and credit assignment during the training phase.\nSolution, train on each task separately. Then train a meta-policy in a supervised manner from earlier policies. Using a Logarithmic Mapping to enable Lower Discount Factors in Reinforcement Learning For Infinite horizon, the gamma decays exponentially. Therefore most of the gammas need to be near 1 to be meaningful for infinite horizon tasks. However, this makes learning hard. Experiments show that with low gamma policies are very bad. Action Gap is defined as difference in Q values for a given state between optimal action and action taken. They prove that with low discount factors the action gap is large.\nThey present the Log(Q) values instead of Q value Better Exploration with Optimistic Actor Critic Being too Greedy is bad. However, Policy gradient methods are greedy. Solution is to use Conservative update which reduces overestimation. However this leads to conservative policies which do not explore much. They provide upper bound to critic values instead of lower bound, which leads to optimistic estimation. They improve of SAC.\nRobust Exploration in Linear Quadratic Reinforcement Learning Robust Exploration $\\rightarrow$ does not cause catastrophic degradation of policy. Targeted Exploration $\\rightarrow$ knowledge which can improve the task solving capabilities. They formulate the exploration-exploitation problem into convex optimization problem.\nTight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies Minmax Regret bounds for Model-based RL based on short term and long term planning. They show that short-term planning (one step planning) is minmax optimal with finite horizon MDPs.\nHindsight Credit Assignment Usually, the credit assignment is done with temporal scale and assume that noisy assignment would eventually be able to learn the long term action-consequences. Man has to figure out what caused him to be wet during the day. As an RL algorithm, he would spend many days getting wet by not bringing an umbrella. They solve this by explicitly learning the relevant credit using posterior probabilities. Weight Agnostic Neural Networks Why are CNNs much better than Fully-connected layers. No matter how much you increase the width of FC layers it would not beat CNNs. The answer lies in the structure of CNNs. This works explores how much far just the structure of Neural Networks can take us without relying on the weights. They create a initial population of structures with connections and then initialize with different weights for each structure. Without training they evaluate the performance of all the structures and pick out top K candidates. Perform 5 additional operations such as addition of nodes, addition of connections etc and do the procedure repeatedly. They show that structure alone can lead to 82% accuracy over MNIST dataset without training.\nA neurally plausible model learns successor representations in partially observable environments This paper was about representation of belief states with he help of distributed distributional code and successor features. They derive the connection between these two from neuroscience. They also generalize this over POMDPs\nDualDICE: Behaviour-agnostic estimation of discounted stationary Distribution Corrections How can we estimate average discounted return for a particular policy for off policy algorithms as the experience collection is done over varied number of policies. They propose a change in the loss function based over ratio of density problem (similar to PPO) with importance of weighting trick.\nVIREL: A Variational Inference Framework for Reinforcement Learning Look at RL as probabilistic inference problem. People have tried Pseudo-likelihood methods (risk taking) and Maximum entropy objective (such as SAC, however they have issue of non-recoverable of optimal policies). They present actor-critic methods with expectation maximization with much better sample efficiency.\nPosters RL  Doubly-Robust Lasso Bandit - Gi Soo Kim, Myunghee Cho Paik - Seoul National University Decentralized cooperative stochastic bandits - David Martinez-Rubio, Varun Kanade, Patrick Rebeschini - Oxford Multi-agent Common knowledge Reinforcement Learning - Christian A. Schroeder de Witt, Jakob N. Foerster, Gregory Farquhar, Philip H. S. Torr, Wendelin Boehmer, Shimon Whiteson Meta-Learning with Warped Gradient Descent - Sebastian Flennerhag, Andrei A. Rusu, Razvan Pascanu, Hujun Yin, Raia Hadsell Measuring the reliability of RL algorithms - Stephanie C.Y. Chan, Sam Fishman, John Canny, Anoop Korattikara, Sergio Guadarrama Adaptive Online Planning for Continual Lifelong Learning - Kevin Lu, Igor Mordatch, Pieter Abbeel Learning Efficient Representations for Intrinsic Motivation - Ruihan Zhao, Stas Tiomkin, Pieter Abbeel Harnessing Structures for value based planning and reinforcement learning - Yuzhe Yang, Guo Zhang, Zhi Xu, Dina Katabi Benchmarking Safe Exploration in DRL - Alex Ray, Joshua Achiam, Dario Amodei Search on the Replay Buffer: Bridging Planning and Reinforcement Learning  NLP  Landmark Ordinal Embedding - Nikhil Ghosh, Yuxin Chen, Yisong Yue Text based interactive recommendation via constraint-augmented reinforcement learning - Ruiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, Changyou Chen Model based Reinforcement learning with adversarial training for online recommendation - Xueying Bai, Jian Guan, Hongning Wang Stochastic shared Embeddings: Data Driven Regularization of Embedding Layers - Liwei Wu, Shuqing Li, Cho-Jui Hsieh, James Sharpnack Quantum Embedding of Knowledge for Reasoning - Dinesh Garg, Shajith Ikbal, Santosh K. Srivastava, Harit Vishwakarma, Hima Karanam, L Venkata Subramaniam Can Unconditional Language Models Recover Arbitrary Sentences? - Nishant Subramani, Sam Bowman, Kyunghyun Cho Interpreting and Improving natural-language processing (in machine) with natural language-processing (in the brain) - Mariya Toneva, Leila Wehbe  Misc  ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for BlackBox Optimization - Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, David Cox High Fidelity Video Prediction with Large Stochastic Recurrent Neural Networks - Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc V. Le, Honglak Lee Adversarial Examples are Not bugs, they are features - Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry Dynamic of stochastic gradient descent for two-layer neural networks in the teacher-student setup - Sebastian Goldt, Madhu S. Advani, Andrew M. Saxe, Florent Krzakala, Lenka Zdeborová Poincare recurrence, cycles and spurious equilibria in gradient-descent-ascent for non-convex non-concave zero-sum games- Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Georgios Piliouras Computational Separations between Sampling and Optimization - Kunal Talwar Learning Imbalanced Datasets with Label-distribution-aware Margin Loss - Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, Tengyu Ma vGraph: A Generative Model for Joint Community Detection and Node Representation Learning - Fan-Yun Sun, Meng Qu, Jordan Hoffmann, Chin-Wei Huang, Jian Tang Putting an End to End-to-End: Gradient Isolated learning of Representations - Sindy Löwe, Peter O\u0026rsquo;Connor, Bastiaan S. Veeling  ","date":1576281600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576281600,"objectID":"1261acdbfd0206c2f44eb32fd8a85e76","permalink":"/post/2019-12-14-neurips-2019/","publishdate":"2019-12-14T00:00:00Z","relpermalink":"/post/2019-12-14-neurips-2019/","section":"post","summary":"I presented my work on Pommerman at Deep Reinforcement Learning Workshop in NeurIPS this year, titled \u0026ldquo;Accelerating training in Pommerman with Imitation and Reinforcement Learning\u0026rdquo;, coauthors are Omkar Shelke, Richa Verma and Harshad Khadilkar.","tags":["Conference","ML"],"title":"NeurIPS-2019 Summary","type":"post"},{"authors":["Hardik Meisheri","Omkar Shelke","Richa Verma","Harshad Khadilkar"],"categories":null,"content":"","date":1573516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573516800,"objectID":"a598d7817657fbcb3d19ac7aa61421f9","permalink":"/publication/accelerating_training_in_pommerman_with_imitation_and_reinforcement_learning/","publishdate":"2019-11-12T00:00:00Z","relpermalink":"/publication/accelerating_training_in_pommerman_with_imitation_and_reinforcement_learning/","section":"publication","summary":"The Pommerman simulation was recently developed to mimic the classic Japanese game Bomberman, and focuses on competitive gameplay in a multi-agent setting. We focus on the 22 team version of Pommerman, developed for a competition at NeurIPS 2018. Our methodology involves training an agent initially through imitation learning on a noisy expert policy, followed by a proximal-policy optimization (PPO) reinforcement learning algorithm. The basic PPO approach is modified for stable transition from the imitation learning phase through reward shaping, action filters based on heuristics, and curriculum learning. The proposed methodology is able to beat heuristic and pure reinforcement learning baselines with a combined 100,000 training games, significantly faster than other non-tree-search methods in literature. We present results against multiple agents provided by the developers of the simulation, including some that we have enhanced. We include a sensitivity analysis over different parameters, and highlight undesirable effects of some strategies that initially appear promising. Since Pommerman is a complex multi-agent competitive environment, the strategies developed here provide insights into several real-world problems with characteristics such as partial observability, decentralized execution (without communication), and very sparse and delayed rewards.","tags":["Reinforcement Learning","Games","Pommerman","Deep Learning"],"title":"Accelerating Training in Pommerman with Imitation and Reinforcement Learning","type":"publication"},{"authors":["Souvik Barat","Harshad Khadilkar","Hardik Meisheri","Vinay Kulkarni","Vinita Baniwal","Prashant Kumar","Monika Gajrani"],"categories":null,"content":"","date":1557273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557273600,"objectID":"804b03e93ad5bfdf7bea76a8f316f8c0","permalink":"/publication/actor_based_simulation_for_closed_loop_control_of_supply_chain_using_reinforcement_learning/","publishdate":"2019-05-08T00:00:00Z","relpermalink":"/publication/actor_based_simulation_for_closed_loop_control_of_supply_chain_using_reinforcement_learning/","section":"publication","summary":"Reinforcement Learning (RL) has achieved a degree of success in control applications such as online gameplay and robotics, but has rarely been used to manage operations of business-critical systems such as supply chains. A key aspect of using RL in the real world is to train the agent before deployment, so as to minimise experimentation in live operation. While this is feasible for online gameplay (where the rules of the game are known) and robotics (where the dynamics are predictable), it is much more difficult for complex systems due to associated complexities, such as uncertainty, adaptability and emergent behaviour. In this paper, we describe a framework for effective integration of a reinforcement learning controller with an actor-based simulation of the complex networked system, in order to enable deployment of the RL agent in the real system with minimal further tuning.","tags":["Reinforcement Learning","Supply Chain","Deep Learning"],"title":"Actor Based Simulation for Closed Loop Control of Supply Chain using Reinforcement Learning","type":"publication"},{"authors":null,"categories":null,"content":"For a long time, I wanted to write a summary of the conference that I attended. Not just to share the new ideas and trends prevalent, also to keep track of the thoughts that I had during the conference. Writing has been one of my weakness, I cannot and do not want to write things down. I am too lazy to write down ideas, and because writing it down concertizes the idea, it loses its charm of being glamorous, just like as any tourist destination nowadays.\nEMNLP-2018 has been a huge conference this year, close to 2500 attendees and around 550 papers making it the largest NLP conference ever. It was by far one of the best conferences that I have attended.\nI wanted to keep track of all the notes and the papers that I would conveniently put in my to read folder, so I thought of writing a blog post on it to summarize the on going trend and the tricks most of paper used recently.\nBroad and quick summary News  EMNLP 2019 will be co-located with IJCNLP Hongkong (Expect Chinese to take over NLP soon) A lot of visa issue happened in Brussels too, one of the Ph.D. students was detained for 5–6 hrs and deported back, keynote speaker for workshop Dilip Rao gave a talk on skype. Next conferences NAACL- Minneapolis, ACL-florence All the talks have been recorded and videos might be up soon.  General trends across the conference  Almost all the papers report that ELMO (Stands for Deep contextualized word representations, won the best paper award in NAACL this year) performs way better than using GLOVE embeddings and most of NLP community has moved to language models. Bi-LSTM + Attention mechanism has been common go to architecture Good number on papers on languages with low representations, such as Spanish, Tamil etc using transfer learning, be it on representation, generation or translation Blackbox NLP was the new workshop organized this year and it features some of the best talks in the conference (Partially due to heavy weights supporting and publicizing it) Keynote by Yoav Goldberg discussed what are the downsides of using RNNs and what specifically it could encode and what it could not. Some papers that I liked - Interpretable Structure Induction via Sparse Attention, I was unaware with the sparse attention and apparently it was talked about in WMT, WASSA and Blackbox NLP a lot. - Understanding Convolutional Neural Networks for Text Classification Did some ablation study on what the CNN is focusing on while doing a sentiment analysis. Most of the time it just focuses on the random thing, they also tried augmenting data with the negation and flipping the labels, interesting paper to check it out.  Keynote talk III The Moment of Meaning and the Future of Computational Semantics by Johan Bos, talked about classical way of doing semantic analysis and some anecdotes on why BLEU is not the correct or rather only metric on which translation and generation should be compared (Simple negation can yield upto 0.9 score), argued that metric should be more rooted and should contain the aspect of what it means. Although his work could not surpass the neural semantic approach.\nBest papers  MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling might be useful for people working on chatbot, they have released a dataset containing dialogues over multiple domains and benchmarked using the current systems. Linguistically-Informed Self-Attention for Semantic Role Labeling, this was total awe aspiring talk, semantic role labeling with multi-headed attention and very deep network. (https://github.com/strubell/LISA) Phrase-Based \u0026amp; Neural Unsupervised Machine Translation, this is kind of neural stylistic transfer just as in image where they transfer artistic styles to the images. But in this, they transfer it across age, gender etc. Cool stuff.  ","date":1548460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548460800,"objectID":"c0cdc2774a61376b8271b18b73a5d47b","permalink":"/post/2019-01-26-emnlp-2018/","publishdate":"2019-01-26T00:00:00Z","relpermalink":"/post/2019-01-26-emnlp-2018/","section":"post","summary":"For a long time, I wanted to write a summary of the conference that I attended. Not just to share the new ideas and trends prevalent, also to keep track of the thoughts that I had during the conference.","tags":["Conference","NLP"],"title":"EMNLP-2018 Summary","type":"post"},{"authors":["Kaustubh Mani","Ishan Verma","Hardik Meisheri","Lipika Dey"],"categories":null,"content":"","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"5648cfdc272d7f3d8a9635e8b131703f","permalink":"/publication/multi-document_summarization_using_distributed_bag-of-words_model/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/publication/multi-document_summarization_using_distributed_bag-of-words_model/","section":"publication","summary":"As the number of documents on the web is growing exponentially, multi-document summarization is becoming more and more important since it can provide the main ideas in a document set in short time. In this paper, we present an unsupervised centroid-based document-level reconstruction framework using distributed bag of words model. Specifically, our approach selects summary sentences in order to minimize the reconstruction error between the summary and the documents. We apply sentence selection and beam search, to further improve the performance of our model. Experimental results on two different datasets show significant performance gains compared with the state-of-the-art baselines.","tags":["Summarization","Deep Learning","NLP"],"title":"Multi-Document Summarization using Distributed Bag-of-Words Model","type":"publication"},{"authors":["Ishan Verma","Rahul Ahuja","Hardik Meisheri","Lipika Dey"],"categories":null,"content":"","date":1543795200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543795200,"objectID":"bf317c8078358c4894718342ed6fc5f7","permalink":"/publication/air_pollutant_severity_prediction_using_bi-directional_lstm_network/","publishdate":"2018-12-03T00:00:00Z","relpermalink":"/publication/air_pollutant_severity_prediction_using_bi-directional_lstm_network/","section":"publication","summary":"Air pollution has emerged as a universal concern across the globe affecting human health. This increasing danger motivates the study of systems for predicting air pollutant severities ahead of time. In this paper, we have proposed the use of a bi-directional LSTM model to predict air pollutant severity levels ahead of time. We have shown that the predictions can be significantly improved using an ensemble of three Bi-Directional LSTMs (BiLSTM) that model the long-term, short-term and immediate effects of PM2.5 (the key air pollutant) severity levels. Further, weather information data has been taken into account while modelling, since they are found to boost prediction accuracies. Experimental results for multiple locations in New Delhi, India are presented to demonstrate model superiority over earlier techniques.","tags":["Time Series Prediction","Deep Learning"],"title":"Air Pollutant Severity Prediction Using Bi-Directional LSTM Network","type":"publication"},{"authors":["Hardik Meisheri","Harshad Khadilkar"],"categories":null,"content":"","date":1540944000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540944000,"objectID":"62d0019f656c1058a7abc331d075c23d","permalink":"/publication/learning_representations_for_sentiment_classification_using_multi-task_framework/","publishdate":"2018-10-31T00:00:00Z","relpermalink":"/publication/learning_representations_for_sentiment_classification_using_multi-task_framework/","section":"publication","summary":"Most of the existing state of the art sentiment classification techniques involve the use of pre-trained embeddings. This paper postulates a generalized representation that collates training on multiple datasets using a Multi-task learning framework. We incorporate publicly available, pre-trained embeddings with Bidirectional LSTM’s to develop the multi-task model. We validate the representations on an independent test Irony dataset that can contain several sentiments within each sample, with an arbitrary distribution. Our experiments show a significant improvement in results as compared to the available baselines for individual datasets on which independent models are trained. Results also suggest superior performance of the representations generated over Irony dataset.","tags":["Sentiment Analysis","Affect","Deep Learning","NLP"],"title":"Learning representations for sentiment classification using Multi-task framework","type":"publication"},{"authors":["Sachin Thukral","Hardik Meisheri","Tushar Kataria","Aman Agarwal","Ishan Verma","Arnab Chatterjee","Lipika Dey"],"categories":null,"content":"","date":1535500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535500800,"objectID":"1f7552a2118ce5387e8e5609d51d5251","permalink":"/publication/analyzing_behavioral_trends_in_community_driven_discussion_platforms_like_reddit/","publishdate":"2018-08-29T00:00:00Z","relpermalink":"/publication/analyzing_behavioral_trends_in_community_driven_discussion_platforms_like_reddit/","section":"publication","summary":"The aim of this paper is to present methods to systematically analyze individual and group behavioral patterns observed in community driven discussion platforms like Reddit where users exchange information and views on various topics of current interest. We conduct this study by analyzing the statistical behavior of posts and modeling user interactions around them. We have chosen Reddit as an example, since it has grown exponentially from a small community to one of the biggest social network platforms in the recent times. Due to its large user base and popularity, a variety of behavior is present among users in terms of their activity. Our study provides interesting insights about a large number of inactive posts which fail to gather attention despite their authors exhibiting Cyborg-like behavior to draw attention. We also present interesting insights about short-lived but extremely active posts emulating a phenomenon like Mayfly Buzz. Further, we present methods to find the nature of activity around highly active posts to determine the presence of Limelight hogging activity, if any. We analyzed over 2 million posts and more than 7 million user responses to them during entire 2008 and over 63 million posts and over 608 million user responses to them from August 2014 to July 2015 amounting to two one-year periods, in order to understand how social media space has evolved over the years.","tags":["Behavioral Analysis","Reddit"],"title":"Analyzing Behavioral Trends in Community Driven Discussion Platforms Like Reddit","type":"publication"},{"authors":["Hardik Meisheri","Lipika Dey"],"categories":null,"content":"","date":1528156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528156800,"objectID":"56037c21d931c77ea5c66596ae66940e","permalink":"/publication/learning-robust-representations-using-multi-attention-architecture/","publishdate":"2018-06-05T00:00:00Z","relpermalink":"/publication/learning-robust-representations-using-multi-attention-architecture/","section":"publication","summary":"This paper presents system description of our submission to the SemEval-2018 task-1, Affect in tweets for the English language. We combine three different features generated using deep learning models and traditional methods in support vector machines to create a unified ensemble system. A robust representation of a tweet is learned using a multi-attention based architecture which uses a mixture of different pre-trained embeddings. In addition to this analysis of different features is also presented. Our system ranked 2nd, 5th, and 7th in different subtasks among 75 teams.","tags":["Sentiment Analysis","Affect","Deep Learning","NLP"],"title":"TCS Research at SemEval-2018 Task 1 Learning Robust Representations using Multi-Attention Architecture","type":"publication"},{"authors":null,"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"909743c8bcfa448b85d409f68a5b00ed","permalink":"/project/reinforcement-learning-in-supply-chain-optimization/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/project/reinforcement-learning-in-supply-chain-optimization/","section":"project","summary":"Applicability of reinforcement learning (RL) algorithms to a class of problems rarely addressed in machine learning literature, involving the control of a dynamic system with high-dimensional control inputs (actions).","tags":["Reinforcement Learning","Supply Chain","Deep Learning"],"title":"Reinforcement Learning in Supply Chain Optimization","type":"project"},{"authors":null,"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"385b32a5a393666d3cfecf115419d29f","permalink":"/project/pommerman/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/project/pommerman/","section":"project","summary":"The Pommerman environment is based on the classic Nintendo console game, Bomberman.","tags":["Reinforcement Learning","Games","Pommerman","Deep Learning"],"title":"Pommerman","type":"project"},{"authors":["Hardik Meisheri","Nagaraj Ramrao","Suman K Mitra"],"categories":null,"content":"","date":1519516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519516800,"objectID":"c9ddc0911725e8168cb1b99ae4c071dc","permalink":"/publication/multiclass_common_spatial_pattern_for_eeg_based_brain_computer_interface_with_adaptive_learning_classifier/","publishdate":"2018-02-25T00:00:00Z","relpermalink":"/publication/multiclass_common_spatial_pattern_for_eeg_based_brain_computer_interface_with_adaptive_learning_classifier/","section":"publication","summary":"In Brain Computer Interface (BCI), data generated from Electroencephalogram (EEG) is non-stationary with low signal to noise ratio and contaminated with artifacts. Common Spatial Pattern (CSP) algorithm has been proved to be effective in BCI for extracting features in motor imagery tasks, but it is prone to overfitting. Many algorithms have been devised to regularize CSP for two class problem, however they have not been effective when applied to multiclass CSP. Outliers present in data affect extracted CSP features and reduces performance of the system. In addition to this non-stationarity present in the features extracted from the CSP present a challenge in classification. We propose a method to identify and remove artifact present in the data during pre-processing stage, this helps in calculating eigenvectors which in turn generates better CSP features. To handle the non-stationarity, Self-Regulated Interval Type-2 Neuro-Fuzzy Inference System (SRIT2NFIS) was proposed in the literature for two class EEG classification problem. This paper extends the SRIT2NFIS to multiclass using Joint Approximate Diagonalization (JAD). The results on standard data set from BCI competition IV shows significant increase in the accuracies from the current state of the art methods for multiclass classification.","tags":["BCI","Neuro-Fuzzy"],"title":"Multiclass Common Spatial Pattern for EEG based Brain Computer Interface with Adaptive Learning Classifier","type":"publication"},{"authors":["Hardik Meisheri","Kunal Ranjan","Lipika Dey"],"categories":null,"content":"","date":1510963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510963200,"objectID":"f2e131ce10c51d0a4fa1c633cb93c738","permalink":"/publication/sentiment_extraction_from_consumer-generated_noisy_short_texts/","publishdate":"2017-11-18T00:00:00Z","relpermalink":"/publication/sentiment_extraction_from_consumer-generated_noisy_short_texts/","section":"publication","summary":"Sentiment analysis or recognizing emotions from short and noisy text from social networks such as twitter has been a challenging task. Most of the existing models use word level embeddings for the final classification of the sentiments. This paper proposes a novel representation of short text derived from a combination of word embeddings and character embeddings using Bidirectional LSTM (BiLSTM). Along with this, we use attention mechanism that learns to focus on sentiment specific words. Robust representation of short text can be applied for sentiment classification as well as predicting intensity of sentiments. This paper presents evaluation of proposed model on classification as well as regression dataset. Results show significant improvement in results as compared to baselines of respective datasets.","tags":["Sentiment Analysis","Affect","Deep Learning","NLP"],"title":"Sentiment extraction from Consumer-generated noisy short texts","type":"publication"},{"authors":["Hardik Meisheri","Rupsa Saha","Priyanka Sinha","Lipika Dey"],"categories":null,"content":"","date":1505001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505001600,"objectID":"c1af367246db568f8d26bae0a70d0432","permalink":"/publication/textmining_at_emoint-2017_a_deep_learning_approach_to_sentiment_intensity_scoring_of_english_tweets/","publishdate":"2017-09-10T00:00:00Z","relpermalink":"/publication/textmining_at_emoint-2017_a_deep_learning_approach_to_sentiment_intensity_scoring_of_english_tweets/","section":"publication","summary":"This paper describes our approach to the Emotion Intensity shared task. A parallel architecture of Convolutional Neural Network (CNN) and Long short term memory networks (LSTM) alongwith two sets of features are extracted which aid the network in judging emotion intensity. Experiments on different models and various features sets are described and analysis on results has also been presented.","tags":["Sentiment Analysis","Affect","Deep Learning","NLP"],"title":"Textmining at EmoInt-2017: A Deep Learning Approach to Sentiment Intensity Scoring of English Tweets","type":"publication"},{"authors":["Ishna Verma","Lipika Dey","Hardik Meisheri"],"categories":null,"content":"","date":1503446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503446400,"objectID":"80fb2f63c2e23138a2b1d1623614d3c2","permalink":"/publication/stock_paper/","publishdate":"2017-08-23T00:00:00Z","relpermalink":"/publication/stock_paper/","section":"publication","summary":"The impact of different types of events reported in News articles on stock market is a widely accepted phenomenon. Market analysts rely heavily on technology to combine data from different sources and generate appropriate insights for predicting stock movements. With plethora of sources reporting news on plentitude of events happening across the world, a combination of text mining techniques and predictive technologies can play a significant role in this arena. In this paper we have presented methodologies to identify and quantify the presence of different types of information that can affect the market from a multitude of web sources, and finally use the information for predicting stock movement direction. We propose the use of PESTEL factors to categorize market-impacting information. We have analyzed large volumes of past available data using Granger causality to understand how these categories impact the market. We propose a paragraph-vector based information classification mechanism. We also present Long-Short term memory Network (LSTM) based prediction model to investigate the prediction capabilities of the information components. The proposed system outperforms state of the art linear SVM on data from different stock indices.","tags":["NLP","Deep Learning"],"title":"Detecting, quantifying and accessing impact of news events on Indian stock indices","type":"publication"},{"authors":null,"categories":null,"content":"","date":1494806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494806400,"objectID":"484c5bd3a4312256c910e9e600170fe0","permalink":"/project/sentiment-analysis-of-short-text/","publishdate":"2017-05-15T00:00:00Z","relpermalink":"/project/sentiment-analysis-of-short-text/","section":"project","summary":"Sentiment analysis or recognizing emotions from short and noisy text typically from social networks such as Twitter.","tags":["Sentiment Analysis","Affect","Deep Learning","NLP"],"title":"Sentiment Analysis of Short text","type":"project"},{"authors":null,"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"c72b37aeed7d0a6bc9b8ec211d95b966","permalink":"/project/behavioral-analysis-of-social-media-posts/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/project/behavioral-analysis-of-social-media-posts/","section":"project","summary":"To analyze the patterns of individual and group behaviour observed in community-driven discussion platforms like Reddit.","tags":["Behavioral Analysis","Reddit"],"title":"Behavioral Analysis of Social media posts","type":"project"},{"authors":null,"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"769b8a155fbe8e3196ca04777a659bca","permalink":"/project/undergrad-thesis/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/project/undergrad-thesis/","section":"project","summary":"In this project, we aim to develop a fully functional prototype of motorized wheelchair which can be controlled by speech, joystick and neck movement.","tags":["Embedded Systems","Speech-Recognition","Micro-controllers"],"title":"Undergrad Thesis - Smart Wheel Chair","type":"project"},{"authors":null,"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"dedc1e3fbd3eb71e57b0fb48abc5773e","permalink":"/project/news-aggreation-and-its-effect-over-stock-and-market/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/project/news-aggreation-and-its-effect-over-stock-and-market/","section":"project","summary":"The impact of different types of events reported in News articles on stock market is a widely accepted phenomenon. Market analysts rely heavily on technology to combine data from different sources and generate appropriate insights for predicting stock movements.","tags":["NLP","Deep Learning"],"title":"News aggreation and its effect over Stock and market","type":"project"},{"authors":["Hardik Meisheri","Nagaraj Ramrao","Suman K Mitra"],"categories":null,"content":"","date":1473033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473033600,"objectID":"1cfbee2efff60ddc6a94eeeeba7a5c69","permalink":"/publication/multiclass_common_spatial_pattern_with_artifacts_removal_methodology_for_eeg_signals/","publishdate":"2016-09-05T00:00:00Z","relpermalink":"/publication/multiclass_common_spatial_pattern_with_artifacts_removal_methodology_for_eeg_signals/","section":"publication","summary":"Common Spatial Pattern (SP) algorithm has been proved to be effective in Brain Computer Interface (BCI) for extracting features from Electroencephalogram (EEG) signals used in motor imagery tasks, but it is vulnerable to noise and the problem of over-fitting. Many algorithms have been devised to regularize CSP for two class problem, however they have not been effective when applied to multiclass CSP. The features extracted using the CSP are non-stationary in nature which increases the difficulty during classification. We propose a method to remove trials that are affected by noise before calculating the CSP. This helps in calculating eigenvectors which generates better CSP. To handle the non-stationarity in the EEG signal, Self-Regulated Interval Type-2 Neuro-Fuzzy Inference System (SRIT2NFIS) was proposed in the literature for two class EEG classification problem. This paper extends the SRIT2NFIS to Multiclass CSP using Joint Approximate Diagonalization (JAD). The results are presented on standard dataset.","tags":["BCI","Neuro-Fuzzy"],"title":"Multiclass common spatial pattern with artifacts removal methodology for EEG signals","type":"publication"},{"authors":null,"categories":null,"content":"","date":1430438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430438400,"objectID":"26375b4faf48283ee4082101d48bf5d4","permalink":"/project/brain-computer-interface/","publishdate":"2015-05-01T00:00:00Z","relpermalink":"/project/brain-computer-interface/","section":"project","summary":"Brain Computer Interface (BCI) provides a pathway for communication from humans to computer without any muscular activity.","tags":["Brain Computer Interface"],"title":"Brain Computer Interface","type":"project"}]