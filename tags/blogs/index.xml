<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs | Hardik</title>
    <link>/tags/blogs/</link>
      <atom:link href="/tags/blogs/index.xml" rel="self" type="application/rss+xml" />
    <description>Blogs</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 04 Apr 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Blogs</title>
      <link>/tags/blogs/</link>
    </image>
    
    <item>
      <title>Musings While Building with LLMs</title>
      <link>/post/2025-04-04-lessons_llm/</link>
      <pubDate>Fri, 04 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/post/2025-04-04-lessons_llm/</guid>
      <description>&lt;h2 id=&#34;building-full-fledged-demos-with-llms-lessons-from-the-trenches&#34;&gt;Building Full-Fledged Demos with LLMs: Lessons from the Trenches&lt;/h2&gt;
&lt;p&gt;When you&amp;rsquo;re working with large language models (LLMs) to bring an idea to life, it&amp;rsquo;s easy to be impressed by how quickly they can write code, generate explanations, or stitch together components. But once you try to build something that actually works end-to-end like a real demo with frontend, backend, integrations, and maybe even some bells and whistles the experience shifts dramatically.&lt;/p&gt;
&lt;p&gt;Recently, I decided to push a fun idea all the way to a functioning demo using LLMs as my main assistants. I relied on them to suggest code, fix bugs, structure the project, and more. Along the way, I discovered what works, what really doesn’t, and a few key practices that made the whole process smoother. More fascinatingly, many of the &amp;ldquo;hacks&amp;rdquo; I stumbled upon seem to be practical workarounds for deep, theoretically understood behaviors of these models.&lt;/p&gt;
&lt;p&gt;Here are some lessons I learned the hard way, backed by a dive into the research explaining why LLMs behave like this.&lt;/p&gt;
&lt;h3 id=&#34;1-always-ask-for-a-directory-structure-right-at-the-start&#34;&gt;1. Always Ask for a Directory Structure Right at the Start&lt;/h3&gt;
&lt;p&gt;Before writing a single line of code, ask the LLM for a full directory structure. Without it, your project becomes a mess. Make the model detail folders, files, and even placeholder comments. This sets the stage and keeps both you and the model aligned on architecture.&lt;/p&gt;
&lt;p&gt;While not directly addressed by attention sink research, this relates to the LLM&amp;rsquo;s limited reasoning capacity and susceptibility to losing track. Providing a clear, stable structure acts as an external &amp;ldquo;anchor&amp;rdquo; or scaffold, helping the model (and you) maintain coherence throughout the development process, much like the &lt;code&gt;`&amp;lt;bos&amp;gt;`&lt;/code&gt; token might anchor attention within a sequence ([1]).&lt;/p&gt;
&lt;h3 id=&#34;2-regenerate-full-files-every-few-steps-to-avoid-drift&#34;&gt;2. Regenerate Full Files Every Few Steps to Avoid Drift&lt;/h3&gt;
&lt;p&gt;LLMs are clever, but they can lose track of the big picture pretty quickly. As conversations go on, they might forget earlier details or start to contradict themselves without realizing it.&lt;/p&gt;
&lt;p&gt;One of the best things you can do is to ask the model to regenerate the entire file after every three or four rounds of updates or modifications. It may feel redundant, but this helps keep everything internally consistent.&lt;/p&gt;
&lt;p&gt;Without this, you will find yourself fixing logic errors that crept in because a function name changed slightly, or an assumption made earlier was silently dropped.&lt;/p&gt;
&lt;p&gt;Treat this like refreshing the page. It helps you avoid stale or corrupted states.&lt;/p&gt;
&lt;p&gt;This directly combats &lt;strong&gt;representational collapse&lt;/strong&gt; and the &lt;strong&gt;&amp;ldquo;lost in the middle&amp;rdquo; phenomenon&lt;/strong&gt;. Research like Barbero et al. (2024, 2025) shows that as sequences get longer or models deeper, token representations can homogenize, making distinct concepts blend together ([1], [2]). Furthermore, studies like Liu et al. (2023) demonstrate that LLMs struggle to recall or accurately use information presented in the middle of long contexts, favoring the beginning and end ([3]). By regenerating the full file, you force the model to re-encode the entire structure and logic based on the &lt;em&gt;current&lt;/em&gt; state, effectively &amp;ldquo;refreshing&amp;rdquo; its internal representation and mitigating the drift caused by these effects.&lt;/p&gt;
&lt;h3 id=&#34;3-when-the-llm-gets-stuck-step-in-and-debug-manually&#34;&gt;3. When the LLM Gets Stuck, Step In and Debug Manually&lt;/h3&gt;
&lt;p&gt;Sometimes the LLM gets stuck in a loop, suggesting similar, ineffective fixes. This is your cue to take over. List potential failure modes, guide the model step-by-step through them, and clearly state what you&amp;rsquo;ve already tried.&lt;/p&gt;
&lt;p&gt;This could be a manifestation of the model getting &amp;ldquo;stuck&amp;rdquo; in suboptimal attention patterns or being overly influenced by earlier (potentially incorrect) parts of the context due to phenomena like &lt;strong&gt;attention sinks&lt;/strong&gt; or &lt;strong&gt;representational collapse&lt;/strong&gt;. It might be unable to distinguish between subtly different states or re-evaluate its approach ([1], [2]). Your manual intervention and precise guidance help break this cycle by forcing it to reconsider specific components or logic paths it might otherwise overlook. Think of yourself as redirecting its focus away from collapsed or overly mixed representations.&lt;/p&gt;
&lt;h3 id=&#34;4-work-around-the-context-window&#34;&gt;4. Work around the context window&lt;/h3&gt;
&lt;p&gt;Even models with huge advertised context windows (like 1 million tokens) often show performance degradation well before that limit. Practical experience often suggests issues start cropping up around the 50k-100k token mark. Plan your workflow accordingly: break tasks into smaller milestones and restart sessions periodically, summarizing progress.&lt;/p&gt;
&lt;p&gt;This aligns perfectly with research on effective context length and the &amp;ldquo;lost in the middle&amp;rdquo; problem ([3]). Studies show that the &lt;em&gt;effective&lt;/em&gt; context length (how much context the model can &lt;em&gt;actually use&lt;/em&gt; reliably) is often much smaller than the &lt;em&gt;training&lt;/em&gt; context length. The U-shaped performance curve (better at ends, worse in the middle) persists even in very long context models ([3]). Techniques like &lt;strong&gt;attention sinks&lt;/strong&gt; help ([1]), but they don&amp;rsquo;t fully solve the problem of utilizing distant information. Restarting sessions is a pragmatic way to avoid hitting the point where information loss becomes critical.&lt;/p&gt;
&lt;h3 id=&#34;5-for-authentication-and-security-trust-external-resources&#34;&gt;5. For Authentication and Security, Trust External Resources&lt;/h3&gt;
&lt;p&gt;LLMs are notoriously bad at generating secure code, especially for critical areas like authentication. Ask for the general flow, but &lt;em&gt;never&lt;/em&gt; trust the implementation details. Rely on official documentation, battle-tested libraries (Firebase, Auth0), and established security best practices.&lt;/p&gt;
&lt;p&gt;Research consistently shows that LLM-generated code often contains vulnerabilities ([5]). Models are trained on vast amounts of public code, including insecure examples. They lack a true understanding of security consequences. Studies report high percentages of misuse or vulnerabilities in generated code ([5]). Furthermore, LLMs can be susceptible to prompt injection, potentially leading to insecure outputs even from seemingly benign prompts ([4]). Security requires nuanced, context-aware reasoning that current LLMs struggle with. Human oversight and reliance on verified resources are non-negotiable here. The OWASP Top 10 for LLM Applications specifically highlights risks like insecure output handling ([4]).&lt;/p&gt;
&lt;h3 id=&#34;theory-backs-it-up-deeper-than-just-quirks&#34;&gt;Theory Backs It Up: Deeper Than Just Quirks&lt;/h3&gt;
&lt;p&gt;As I dug deeper, I realized these practical frustrations weren&amp;rsquo;t just random glitches. They stem from fundamental aspects of how Transformer architectures process information. The Barbero et al. (2025) paper on &lt;strong&gt;attention sinks&lt;/strong&gt; was a key insight ([1]). It argues that the tendency for LLMs to heavily attend to the first token (like &lt;code&gt;`&amp;lt;bos&amp;gt;`&lt;/code&gt;) isn&amp;rsquo;t a bug, but a learned feature to combat &lt;strong&gt;over-mixing&lt;/strong&gt; and &lt;strong&gt;representational collapse&lt;/strong&gt;. By anchoring attention, the model slows down information propagation, helping maintain distinctness between token representations, especially over long sequences or deep layers.&lt;/p&gt;
&lt;p&gt;This connects directly to my experiences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Regenerating files:&lt;/strong&gt; A way to manually fight &lt;strong&gt;representational collapse&lt;/strong&gt; ([1], [2]) and the &amp;ldquo;lost in the middle&amp;rdquo; effect ([3]).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Restarting sessions:&lt;/strong&gt; Acknowledging the limitations of effective context length and preventing catastrophic information loss.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Manual debugging:&lt;/strong&gt; Intervening when the model&amp;rsquo;s internal state becomes too mixed or anchored incorrectly.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It turns out, my pragmatic &amp;ldquo;hacks&amp;rdquo; were intuitive responses to measurable, theoretically understood limitations in how LLMs handle information flow and context.&lt;/p&gt;
&lt;h3 id=&#34;wrapping-up&#34;&gt;Wrapping Up&lt;/h3&gt;
&lt;p&gt;Building complex applications with LLMs is an exercise in navigating these inherent limitations. They are incredibly powerful assistants, but not infallible oracles. Understanding &lt;em&gt;why&lt;/em&gt; they sometimes fail—due to mechanisms like &lt;strong&gt;attention sinks&lt;/strong&gt;, &lt;strong&gt;representational collapse&lt;/strong&gt;, or context window effects—makes it easier to anticipate problems and apply effective workarounds.&lt;/p&gt;
&lt;p&gt;Guide the LLM, correct it often, verify its outputs (especially security-critical code!), and don&amp;rsquo;t hesitate to take the wheel. Let the models do the heavy lifting, but remember: you&amp;rsquo;re still the engineer steering the ship.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Barbero, F., Arroyo, Á., Gu, X., Perivolaropoulos, C., Bronstein, M., Velicković, P., &amp;amp; Pascanu, R. (2025). &lt;em&gt;Why do LLMs attend to the first token?&lt;/em&gt; arXiv preprint 
&lt;a href=&#34;https://arxiv.org/abs/2504.02732&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2504.02732&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Barbero, F., Banino, A., Kapturowski, S., Kumaran, D., et al. (2024). &lt;em&gt;Transformers need glasses! information over-squashing in language tasks&lt;/em&gt;. Advances in Neural Information Processing Systems (NeurIPS). (
&lt;a href=&#34;https://openreview.net/forum?id=Q0Q3N4QjXk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Liu, N. F., Lin, K., Hewitt, J., et al. (2023). &lt;em&gt;Lost in the Middle: How Language Models Use Long Contexts&lt;/em&gt;. arXiv preprint 
&lt;a href=&#34;https://arxiv.org/abs/2307.03172&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2307.03172&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;OWASP. &lt;em&gt;OWASP Top 10 for Large Language Model Applications&lt;/em&gt;. (
&lt;a href=&#34;https://owasp.org/www-project-top-10-for-large-language-model-applications/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Li, Z., et al. (2023). &lt;em&gt;Can ChatGPT Replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation&lt;/em&gt;. arXiv preprint 
&lt;a href=&#34;https://arxiv.org/abs/2308.10335&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2308.10335&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Useful courses/blogs related to Machine Learning</title>
      <link>/post/2020-09-21-interesting_blogs/</link>
      <pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-09-21-interesting_blogs/</guid>
      <description>&lt;p&gt;&lt;strong&gt;List of blogs are informative for RL/ML&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://blog.ml.cmu.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning Blog at CMU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://bair.berkeley.edu/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Berkeley Artificial Intelligence Research Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://ai.stanford.edu/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Standford AI Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://lilianweng.github.io/lil-log/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lilian Weng blog on RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://deepmind.com/blog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Mind Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Lectures/courses for Machine Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://docs.microsoft.com/en-gb/learn/paths/create-machine-learn-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Create machine learning models - Microsoft&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanford CS229: Machine Learning - Andrew Ng&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Linear Regression and Gradient Descent&lt;/li&gt;
&lt;li&gt;Logistic Regression&lt;/li&gt;
&lt;li&gt;Naive Bayes&lt;/li&gt;
&lt;li&gt;SVMs&lt;/li&gt;
&lt;li&gt;Kernels&lt;/li&gt;
&lt;li&gt;Decision Trees&lt;/li&gt;
&lt;li&gt;Introduction to Neural Networks&lt;/li&gt;
&lt;li&gt;Debugging ML Models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/ml-intro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning Crash Course - Google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://course18.fast.ai/ml.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Machine Learning for Coders - Jeremy Howard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://bloomberg.github.io/foml/#homeworkslave&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Foundations of Machine Learning - Bloomberg ML EDU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.youtube.com/playlist?list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tabular Data - Machine Learning University&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLTKMiZHVd_2KyGirGEvKlniaWeLOHhUF3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stat 451: Intro to Machine Learning (Fall 2020) - Sebastain Raschka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Mind x UCL, Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NYU Deep Learning SP21&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Neural Nets: rotation and squashing&lt;/li&gt;
&lt;li&gt;Latent Variable Energy Based Models&lt;/li&gt;
&lt;li&gt;Unsupervised Learning&lt;/li&gt;
&lt;li&gt;Generative Adversarial Networks&lt;/li&gt;
&lt;li&gt;Autoencoders&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
