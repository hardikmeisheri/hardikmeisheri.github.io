<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs | Hardik</title>
    <link>/tags/blogs/</link>
      <atom:link href="/tags/blogs/index.xml" rel="self" type="application/rss+xml" />
    <description>Blogs</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 04 Apr 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Blogs</title>
      <link>/tags/blogs/</link>
    </image>
    
    <item>
      <title>Musings While Building with LLMs</title>
      <link>/post/2025-04-04-lessons_llm/</link>
      <pubDate>Fri, 04 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/post/2025-04-04-lessons_llm/</guid>
      <description>&lt;h2 id=&#34;building-full-fledged-demos-with-llms-lessons-from-the-trenches&#34;&gt;Building Full-Fledged Demos with LLMs: Lessons from the Trenches&lt;/h2&gt;
&lt;p&gt;When you&amp;rsquo;re working with large language models (LLMs) to bring an idea to life, it&amp;rsquo;s easy to be impressed by how quickly they can write code, generate explanations, or stitch together components. But once you try to build something that actually works end-to-end like a real demo with frontend, backend, integrations, and maybe even some bells and whistles the experience shifts dramatically.&lt;/p&gt;
&lt;p&gt;Recently, I decided to push a fun idea all the way to a functioning demo using LLMs as my main assistants. I relied on them to suggest code, fix bugs, structure the project, and more. Along the way, I discovered what works, what really does not, and a few key practices that made the whole process smoother.&lt;/p&gt;
&lt;p&gt;Here are some lessons I learned the hard way, so you hopefully do not have to.&lt;/p&gt;
&lt;h3 id=&#34;1-always-ask-for-a-directory-structure-right-at-the-start&#34;&gt;1. Always Ask for a Directory Structure Right at the Start&lt;/h3&gt;
&lt;p&gt;Before you write a single line of code, ask the LLM to give you a full directory structure. This might seem like an obvious step, but it sets the stage for everything that follows. Without it, your project will likely end up feeling like a messy desk drawer where nothing is in the right place.&lt;/p&gt;
&lt;p&gt;Make the model walk through what folders are needed, what files should go in each folder, and even ask it to include placeholder comments inside the files. This helps you and the model stay aligned on the architecture and responsibilities of each part of the system.&lt;/p&gt;
&lt;p&gt;You are not just writing code. You are designing a system. Get that system layout early and refer back to it often.&lt;/p&gt;
&lt;h3 id=&#34;2-regenerate-full-files-every-few-steps-to-avoid-drift&#34;&gt;2. Regenerate Full Files Every Few Steps to Avoid Drift&lt;/h3&gt;
&lt;p&gt;LLMs are clever, but they can lose track of the big picture pretty quickly. As conversations go on, they might forget earlier details or start to contradict themselves without realizing it.&lt;/p&gt;
&lt;p&gt;One of the best things you can do is to ask the model to regenerate the entire file after every three or four rounds of updates or modifications. It may feel redundant, but this helps keep everything internally consistent.&lt;/p&gt;
&lt;p&gt;Without this, you will find yourself fixing logic errors that crept in because a function name changed slightly, or an assumption made earlier was silently dropped.&lt;/p&gt;
&lt;p&gt;Treat this like refreshing the page. It helps you avoid stale or corrupted states.&lt;/p&gt;
&lt;h3 id=&#34;3-when-the-llm-gets-stuck-step-in-and-debug-manually&#34;&gt;3. When the LLM Gets Stuck, Step In and Debug Manually&lt;/h3&gt;
&lt;p&gt;There will be moments when the LLM just cannot seem to figure out what is wrong. It will keep suggesting similar fixes, none of which solve the problem. This is your cue to stop relying on the model and start debugging like a human.&lt;/p&gt;
&lt;p&gt;Here’s what worked for me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I made a list of all the potential failure modes, even the unlikely ones.&lt;/li&gt;
&lt;li&gt;I asked the LLM to walk through each one step by step, instead of guessing.&lt;/li&gt;
&lt;li&gt;I clearly told the model where I had already looked, so it did not waste time going in circles.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In these situations, your job is to guide the model with precision. If you do not, it will happily spin its wheels forever.&lt;/p&gt;
&lt;p&gt;Think of yourself as the engineer, and the LLM as your very eager intern who needs clear direction.&lt;/p&gt;
&lt;h3 id=&#34;4-work-around-the-context-window&#34;&gt;4. Work around the context window&lt;/h3&gt;
&lt;p&gt;LLMs operate within a limited context window, which is basically the amount of information they can keep track of at any given time. While recent models have made impressive progress on long context handling (Gemini even pushing context windows up to one million tokens), most people working with these models agree that the performance starts to slip as you go deeper into long contexts.&lt;/p&gt;
&lt;p&gt;Somewhere around the fifty to hundred thousand token mark, you start seeing issues. The model begins mixing things up, losing track of earlier logic, or introducing bugs that seem to come out of nowhere.&lt;/p&gt;
&lt;p&gt;You need to plan your workflow with this in mind.&lt;/p&gt;
&lt;p&gt;Try to break your project into smaller, self-contained milestones that can be completed within a reasonable context window. Once you’re getting close to that upper limit, or if the model starts acting weird, start a fresh session. Summarize what you’ve done, maybe copy over some key files or notes, and keep going from there.&lt;/p&gt;
&lt;p&gt;It’s a small habit that can save you from a lot of mysterious, hard-to-debug problems later.&lt;/p&gt;
&lt;h3 id=&#34;5-for-authentication-and-security-trust-external-resources&#34;&gt;5. For Authentication and Security, Trust External Resources&lt;/h3&gt;
&lt;p&gt;One major weak spot for LLMs is anything related to authentication, security, or secure integration. They will give you code, yes, but it is often outdated, insecure, or just plain wrong.&lt;/p&gt;
&lt;p&gt;I learned to ask the LLM for a rough sketch of what I needed like the general flow of an auth system or how tokens should be passed but I never trusted it with the implementation.&lt;/p&gt;
&lt;p&gt;Instead, I looked up official docs, used battle-tested services like Firebase or Auth0, and followed community best practices.&lt;/p&gt;
&lt;p&gt;Security is one area where human oversight is absolutely essential.&lt;/p&gt;
&lt;h3 id=&#34;theory-backs-it-up-why-llms-behave-like-this&#34;&gt;Theory Backs It Up: Why LLMs Behave Like This&lt;/h3&gt;
&lt;p&gt;As it turns out, some of the odd behaviors I encountered while working with LLMs are not just quirks or artifacts of incomplete training, they are actually rooted in the underlying mechanics of how these models process sequences.&lt;/p&gt;
&lt;p&gt;I recently came across a fascinating paper by Barbero et al. (2025) titled 
&lt;a href=&#34;https://arxiv.org/abs/2504.02732&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Why do LLMs attend to the first token?&lt;/em&gt;&lt;/a&gt;, which explores a phenomenon known as attention sinks. In essence, many transformer-based language models exhibit a behavior where a disproportionately high amount of attention is directed toward the very first token in the sequence. This is often the special &lt;code&gt;&amp;lt;bos&amp;gt;&lt;/code&gt; or beginning-of-sequence token.&lt;/p&gt;
&lt;p&gt;At first glance, this may seem wasteful or like a design flaw. However, the authors present a compelling argument that this behavior is not only intentional but also serves a critical role in how LLMs manage information across long contexts.&lt;/p&gt;
&lt;p&gt;Their main idea is that LLMs are prone to what is known as over-mixing or representational collapse. When a model processes a long sequence or operates at great depth, it tends to homogenize the representations of tokens. In simpler terms, the distinctiveness of individual tokens starts to get lost. This leads to a phenomenon where all token embeddings start to look the same, which makes the model forgetful or erratic.&lt;/p&gt;
&lt;p&gt;To counteract this, the model learns to create an anchor by heavily attending to the first token. This acts like a stabilizer that slows down how quickly information gets mixed and propagated through the network. It is a self-learned mechanism to retain structure and clarity in representations, especially in very long sequences.&lt;/p&gt;
&lt;p&gt;Now, here is the fun part. In my experience, I noticed that regenerating entire files periodically, restarting sessions after long stretches of interaction, and manually identifying the source of bugs were all very effective strategies. At the time, these felt like pragmatic hacks. But when viewed through the lens of this paper, they look a lot more like intuitive workarounds for problems that the model itself is also trying to solve.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;My habit of regenerating full files every few turns aligns with the idea of resisting representational collapse. By starting fresh, I was forcing the model to re-encode the correct structure and logic, preventing subtle drift.&lt;/li&gt;
&lt;li&gt;Restarting the session after approximately fifty thousand tokens of interaction helped avoid vestigial context errors, which the paper describes as a symptom of over-squashing and token signal decay.&lt;/li&gt;
&lt;li&gt;When the model got stuck suggesting the same broken fix repeatedly, it was often a result of it being too anchored to earlier tokens or unable to distinguish between closely mixed representations. Taking over the debugging process and directing its focus elsewhere gave it the nudge it needed to break free from that loop.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In hindsight, I was not just patching things up. I was, unknowingly, applying a form of interpretability-aware engineering. The theoretical research just validated it.&lt;/p&gt;
&lt;p&gt;This paper provided a comforting sense of clarity. It explained why the LLM was doing what it was doing and reassured me that the workarounds I used were not just stopgaps, they were intelligent responses to real, measurable behaviors in large language models.&lt;/p&gt;
&lt;p&gt;In hindsight, I was not just patching things up. I was, unknowingly, applying a form of interpretability-aware engineering. The theoretical research just validated it.&lt;/p&gt;
&lt;p&gt;This paper provided a comforting sense of clarity. It explained why the LLM was doing what it was doing and reassured me that the workarounds I used were not just stopgaps, they were intelligent responses to real, measurable behaviors in large language models.&lt;/p&gt;
&lt;h3 id=&#34;wrapping-up&#34;&gt;Wrapping Up&lt;/h3&gt;
&lt;p&gt;If you are trying to go from idea to working demo using LLMs, you are going to hit roadblocks. But with the right strategies, you can avoid many of the common pitfalls and move much faster.&lt;/p&gt;
&lt;p&gt;Use the LLM as a powerful assistant, not as an all-knowing oracle. Guide it, correct it, and do not be afraid to take the wheel when needed.&lt;/p&gt;
&lt;p&gt;Let the models do the heavy lifting, but remember. You are still the one steering the ship.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;Barbero, F., Arroyo, Á., Gu, X., Perivolaropoulos, C., Bronstein, M., Velicković, P., &amp;amp; Pascanu, R. (2025). &lt;em&gt;Why do LLMs attend to the first token?&lt;/em&gt; arXiv preprint 
&lt;a href=&#34;https://arxiv.org/abs/2504.02732&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv:2504.02732&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Useful courses/blogs related to Machine Learning</title>
      <link>/post/2020-09-21-interesting_blogs/</link>
      <pubDate>Mon, 21 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-09-21-interesting_blogs/</guid>
      <description>&lt;p&gt;&lt;strong&gt;List of blogs are informative for RL/ML&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://blog.ml.cmu.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning Blog at CMU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://bair.berkeley.edu/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Berkeley Artificial Intelligence Research Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://ai.stanford.edu/blog/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Standford AI Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://lilianweng.github.io/lil-log/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lilian Weng blog on RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://deepmind.com/blog&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Mind Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Lectures/courses for Machine Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://docs.microsoft.com/en-gb/learn/paths/create-machine-learn-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Create machine learning models - Microsoft&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanford CS229: Machine Learning - Andrew Ng&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Linear Regression and Gradient Descent&lt;/li&gt;
&lt;li&gt;Logistic Regression&lt;/li&gt;
&lt;li&gt;Naive Bayes&lt;/li&gt;
&lt;li&gt;SVMs&lt;/li&gt;
&lt;li&gt;Kernels&lt;/li&gt;
&lt;li&gt;Decision Trees&lt;/li&gt;
&lt;li&gt;Introduction to Neural Networks&lt;/li&gt;
&lt;li&gt;Debugging ML Models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://developers.google.com/machine-learning/crash-course/ml-intro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Machine Learning Crash Course - Google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://course18.fast.ai/ml.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Machine Learning for Coders - Jeremy Howard&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://bloomberg.github.io/foml/#homeworkslave&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Foundations of Machine Learning - Bloomberg ML EDU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.youtube.com/playlist?list=PL8P_Z6C4GcuVQZCYf_ZnMoIWLLKGx9Mi2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tabular Data - Machine Learning University&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLTKMiZHVd_2KyGirGEvKlniaWeLOHhUF3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stat 451: Intro to Machine Learning (Fall 2020) - Sebastain Raschka&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Mind x UCL, Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NYU Deep Learning SP21&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Neural Nets: rotation and squashing&lt;/li&gt;
&lt;li&gt;Latent Variable Energy Based Models&lt;/li&gt;
&lt;li&gt;Unsupervised Learning&lt;/li&gt;
&lt;li&gt;Generative Adversarial Networks&lt;/li&gt;
&lt;li&gt;Autoencoders&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
