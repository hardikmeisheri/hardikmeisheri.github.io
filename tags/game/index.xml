<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Game | Academic</title>
    <link>/tags/game/</link>
      <atom:link href="/tags/game/index.xml" rel="self" type="application/rss+xml" />
    <description>Game</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 12 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Game</title>
      <link>/tags/game/</link>
    </image>
    
    <item>
      <title>Accelerating Training in Pommerman with Imitation and Reinforcement Learning</title>
      <link>/publication/accelerating_training_in_pommerman_with_imitation_and_reinforcement_learning/2019-11-12-accelerating_training_in_pommerman_with_imitation_and_reinforcement_learning/</link>
      <pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/publication/accelerating_training_in_pommerman_with_imitation_and_reinforcement_learning/2019-11-12-accelerating_training_in_pommerman_with_imitation_and_reinforcement_learning/</guid>
      <description>&lt;p&gt;The Pommerman simulation was recently developed to mimic the classic Japanese game Bomberman, and focuses on competitive gameplay in a multi-agent setting. We focus on the 22 team version of Pommerman, developed for a competition at NeurIPS 2018. Our methodology involves training an agent initially through imitation learning on a noisy expert policy, followed by a proximal-policy optimization (PPO) reinforcement learning algorithm. The basic PPO approach is modified for stable transition from the imitation learning phase through reward shaping, action filters based on heuristics, and curriculum learning. The proposed methodology is able to beat heuristic and pure reinforcement learning baselines with a combined 100,000 training games, significantly faster than other non-tree-search methods in literature. We present results against multiple agents provided by the developers of the simulation, including some that we have enhanced. We include a sensitivity analysis over different parameters, and highlight undesirable effects of some strategies that initially appear promising. Since Pommerman is a complex multi-agent competitive environment, the strategies developed here provide insights into several real-world problems with characteristics such as partial observability, decentralized execution (without communication), and very sparse and delayed rewards.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;http://academicpages.github.io/files/paper2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download paper here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
